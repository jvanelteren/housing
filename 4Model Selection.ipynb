{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "_NDWSAWprShN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import altair as alt\n",
    "# from aoc import timeit\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# np.random.seed(112)\n",
    " \n",
    "def load(filename):\n",
    "    f = open(filename,\"rb\")\n",
    "    return pickle.load(f)\n",
    "    \n",
    "def save(model, filename='bestmodel.pickle'):\n",
    "    with open('output/'+filename, 'wb') as handle:\n",
    "        pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_feature_selection(cols, filename='feat_selection.pickle'):\n",
    "    with open('output/'+filename, 'wb') as handle:\n",
    "        pickle.dump(cols, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def submit(model, filename='submission.csv'):\n",
    "    pred = model.predict(final_test)\n",
    "    final_test['SalePrice'] = np.exp(pred)\n",
    "    final_test[['Id','SalePrice']].to_csv('output/'+filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "colab_type": "code",
    "id": "3whduyAbrShU",
    "outputId": "ee37d5cf-5d0f-4de2-c51b-9a622e6998a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, final_test, num_x, cat_x, cat_x_ind = load(\"output/engineered_datasets.pickle\")\n",
    "cols = load(\"output/feat_selection.pickle\")\n",
    "feat_selector = load('output/feat_selector.pickle')\n",
    "# results = load(\"output/hyperparam_tuning16selected1800.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = train_x[cols]\n",
    "# df['y'] = train_y\n",
    "# df.to_csv('housing_after_preprocessing.csv')\n",
    "final_test.to_csv('final_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "BPXAZImebs9C"
   },
   "source": [
    "## Import preprocessing pipelines & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from utils.sklearn_custom_steps import DFSimpleImputer, DFOneHotEncoder,DFMinMaxScaler,DFColumnTransformer,DFOutlierExtractor,DFOutlierExtractor,DFStandardScaler,DFRobustScaler,DFSmartImputer, DFPowerTransformer\n",
    "from utils.sklearn_custom_steps import get_pipeline\n",
    "from utils.model_hyperparameters import models,AutoCatBoostRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet,SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import cross_validate,cross_val_score,KFold,GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_models(to_test,train_x=train_x,**kwargs):\n",
    "    for name in to_test:\n",
    "        print(f\"{name.ljust(20)}\", end = ': ')\n",
    "        pipe = get_pipeline(models[name].model, **models[name].preprocess, **kwargs)\n",
    "        test_pipeline(pipe, train_x = train_x)\n",
    "         \n",
    "def test_model(model,train_x = train_x,param=None):\n",
    "    if not param: param = {}\n",
    "    pipe = get_pipeline(model,**param)\n",
    "    return test_pipeline(pipe, train_x=train_x)\n",
    "\n",
    "def test_pipeline(pipe,train_x = train_x):\n",
    "    # print(train_x.shape)\n",
    "    n_fold = 5\n",
    "    scores = cross_validate(pipe, train_x, train_y, scoring='neg_root_mean_squared_error', cv= KFold(n_splits=n_fold,shuffle=True,random_state=64), return_train_score=True)\n",
    "    print(f\"test {-1 * sum(scores['test_score'])/n_fold:.7f}, train {-1 * sum(scores['train_score'])/n_fold:.7f}\")\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to hyperparameter search on preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(model_name, results):\n",
    "    model = get_pipeline(models[model_name].model, **models[model_name].preprocess)\n",
    "    model.set_params(**results[model_name].best_params_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "start AutoCatBoostRegressor\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  9.7min finished\nstart ElasticNet\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  2.3min finished\nstart KernelRidge\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  1.9min finished\nstart Lasso\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  1.8min finished\nstart xgb.XGBRegressor\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed: 35.6min finished\nstart lgb.LGBMRegressor\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  3.5min finished\n"
    }
   ],
   "source": [
    "def hyperparam_search_pipeline(model_name, pipe):\n",
    "   print('start', model_name)\n",
    "   param_grid = {\n",
    "      'preprocess__col_trans__numeric__scale_num' : [DFStandardScaler(),DFRobustScaler(),DFMinMaxScaler(),DFPowerTransformer()],\n",
    "      'preprocess__col_trans__numeric__impute_num__strategy': ['mean','median','most_frequent'],\n",
    "      'preprocess__col_trans__category__impute_cat__strategy': ['most_frequent','constant']}\n",
    "   search = GridSearchCV(pipe, param_grid, cv=KFold(n_splits=5,shuffle=True,random_state=64),scoring='neg_root_mean_squared_error',verbose=1).fit                               (train_x[cols], train_y)\n",
    "   frame =pd.DataFrame(search.cv_results_)\n",
    "   frame.sort_values(by='rank_test_score', inplace=True)\n",
    "   return frame\n",
    "pipe_search = dict()\n",
    "for model_name in results:\n",
    "   pipe_search[model_name] = hyperparam_search_pipeline(model_name, get_estimator(model_name, results))\n",
    "   save(pipe_search,  'hyperparam_pipe64.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "AutoCatBoostRegressor\nbest score -0.11569338613600308\nbest params constant median DFRobustScaler\nElasticNet\nbest score -0.11926819969637574\nbest params constant mean DFStandardScaler\nKernelRidge\nbest score -0.1175124522869253\nbest params constant median DFStandardScaler\nLasso\nbest score -0.11820381952839858\nbest params most_frequent median DFRobustScaler\nxgb.XGBRegressor\nbest score -0.12121163496613555\nbest params most_frequent most_frequent DFRobustScaler\nlgb.LGBMRegressor\nbest score -0.12123153728556438\nbest params constant median DFRobustScaler\n"
    }
   ],
   "source": [
    "pipe_search = load('output/hyperparam_pipe64.pickle')\n",
    "for model_name, res in pipe_search.items():\n",
    "    res = res.reset_index()\n",
    "    print(model_name)\n",
    "    print('best score', res['mean_test_score'][0])\n",
    "    print('best params', \n",
    "        res['param_preprocess__col_trans__category__impute_cat__strategy'][0],\n",
    "        res['param_preprocess__col_trans__numeric__impute_num__strategy'][0],\n",
    "        res['param_preprocess__col_trans__numeric__scale_num'][0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Test of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "AutoCatBoostRegressor\ntest 0.1210077, train 0.0497454\nElasticNet\ntest 0.1294588, train 0.1032053\nKernelRidge\ntest 0.1306891, train 0.1062492\nLasso\ntest 0.1321611, train 0.1046623\nxgb.XGBRegressor\ntest 0.1240356, train 0.0472714\nlgb.LGBMRegressor\ntest 0.1256687, train 0.0652470\n"
    }
   ],
   "source": [
    "# selected columns dataset\n",
    "for model_name in results:\n",
    "    print(model_name)\n",
    "    test_pipeline(get_estimator(model_name, results),train_x=train_x[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "wO5qirPMBqGX"
   },
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV, callbacks\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "NUM_ITERATIONS = 150\n",
    "NO_IMPROVEMENT_STOP_THRES = 25\n",
    "\n",
    "def gen_opt_settings(model_name):\n",
    "    model = {'model': [models[model_name].model]}\n",
    "    for k,v in models[model_name].hyper.items():\n",
    "        model['model__'+k] = v\n",
    "    if models[model_name].hyper:\n",
    "        return (model, NUM_ITERATIONS)\n",
    "    else:\n",
    "        return (model, 1)\n",
    "\n",
    "def optimize_model(model_name,train_x = train_x, train_time = 600):\n",
    "    print('running', model_name)\n",
    "    def no_improvement_detector(optim_result):\n",
    "        score = opt.best_score_\n",
    "        # print(optim_result.x)\n",
    "        print(f\"{'best score':15}{score}\")\n",
    "        if score > opt.train_status['current_score']:\n",
    "            opt.train_status['current_score'] = score\n",
    "            opt.train_status['not_improving'] = 0\n",
    "        else:\n",
    "            opt.train_status['not_improving'] += 1\n",
    "            if opt.train_status['not_improving'] == opt.train_status['stop_thres']: return True\n",
    "    checkpointsaver = callbacks.CheckpointSaver(\"output/\" + model_name + \"_skopt.pkl\")\n",
    "    deadlinestopper = callbacks.DeadlineStopper(train_time)\n",
    "\n",
    "    opt = BayesSearchCV(\n",
    "        get_pipeline(models[model_name].model, **models[model_name].preprocess),\n",
    "        [gen_opt_settings(model_name)],\n",
    "        cv=KFold(n_splits=5,shuffle=True,random_state=64), \n",
    "        scoring = 'neg_root_mean_squared_error',\n",
    "        return_train_score = True,\n",
    "        random_state = 111,\n",
    "        refit=False\n",
    "        )\n",
    "    opt.train_status = { 'current_score': -100, 'not_improving': 0, 'stop_thres' :NO_IMPROVEMENT_STOP_THRES}\n",
    "    opt.fit(train_x,train_y, callback = [no_improvement_detector,checkpointsaver,deadlinestopper])\n",
    "    return opt\n",
    "\n",
    "def hashing(self): return 8398398478478 \n",
    "CatBoostRegressor.__hash__ = hashing # otherwise skopt flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#summarize tuning results\n",
    "def print_results(results):\n",
    "    for model in results:\n",
    "        best_run = results[model].cv_results_['rank_test_score'].index(1)\n",
    "        mean_test_score = -1 * results[model].cv_results_['mean_test_score'][best_run]\n",
    "        std_test_score = results[model].cv_results_['std_test_score'][best_run]\n",
    "        mean_train_score = -1 * results[model].cv_results_['mean_train_score'][best_run]\n",
    "        mean_score_time = results[model].cv_results_['mean_score_time'][best_run]\n",
    "        best_params = results[model].best_params_\n",
    "        print(f\"{model:<30} Best score {mean_test_score:.4f} std {std_test_score:.4f} train {mean_train_score:.4f} time {mean_score_time:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "AutoCatBoostRegressor          Best score 0.1129 std 0.0077 train 0.0496 time 0.0332\nElasticNet                     Best score 0.1159 std 0.0111 train 0.1034 time 0.1005\nKernelRidge                    Best score 0.1162 std 0.0100 train 0.1065 time 0.1085\nLasso                          Best score 0.1153 std 0.0112 train 0.1050 time 0.1251\nxgb.XGBRegressor               Best score 0.1160 std 0.0105 train 0.0474 time 0.0592\nlgb.LGBMRegressor              Best score 0.1184 std 0.0088 train 0.0665 time 0.0650\n"
    }
   ],
   "source": [
    "results = load(\"output/hyperparam_tuning41selected1800.pickle\")\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Stacking best models from hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "def get_estimator(model_name, results,rand_seed):\n",
    "    model = get_pipeline(models[model_name].model, **models[model_name].preprocess)\n",
    "    par = {k:v for k,v in results[model_name].best_params_.items() if k not in ['random_seed','random_state','model']}\n",
    "    model.set_params(**par)\n",
    "    \n",
    "    if model_name != 'KernelRidge':\n",
    "        model.named_steps['model'].set_params(random_state=rand_seed)\n",
    "        # print(model.named_steps['model'].get_params())\n",
    "    return model\n",
    "\n",
    "def get_stacked_model(results,train_x=train_x,rand_seed=42):\n",
    "    to_stack_list = [\n",
    "        'AutoCatBoostRegressor',\n",
    "        'ElasticNet',\n",
    "        'KernelRidge',\n",
    "        'Lasso',\n",
    "        'xgb.XGBRegressor',\n",
    "        'lgb.LGBMRegressor']\n",
    "\n",
    "    # to_stack_list = to_test\n",
    "    # to_stack = [(model_name, results[model_name].best_estimator_) for model_name in to_stack_list]\n",
    "    # to_stack = [(model_name, results[model_name].best_estimator_) for model_name in results]\n",
    "    to_stack = [(model_name, get_estimator(model_name, results,rand_seed)) for model_name in to_stack_list]\n",
    "    model = StackingRegressor(to_stack, final_estimator = (LinearRegression()), passthrough = False)\n",
    "    scores = cross_validate(model, train_x, train_y, scoring='neg_root_mean_squared_error', cv=KFold(n_splits=5,shuffle=True,random_state=rand_seed), return_train_score=True)\n",
    "    print(f\"stacking model train {-1 * sum(scores['train_score'])/5:.4f}, test {-1 * sum(scores['test_score'])/5:.4f}\")\n",
    "    model.fit(train_x,train_y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending, to get rid of some of the overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def blend(model, filename):\n",
    "    preds = [estimator.predict(final_test) for estimator in model.estimators_]\n",
    "    # weights = np.array([0.2]*len(preds) + [(1-len(preds)*0.2)])\n",
    "    weights = np.array([1/6]*len(preds))\n",
    "    print(weights)\n",
    "    # preds.append(model.predict(final_test))\n",
    "    print(len(preds))\n",
    "    # weigh the individual models with 0.1 and the stacked regressor with the remainder\n",
    "    weighted_preds = preds * weights[:, None]\n",
    "    final_preds = np.sum(weighted_preds,axis=0)\n",
    "    final_test['SalePrice'] = np.exp(final_preds)\n",
    "    final_test[['Id','SalePrice']].to_csv('output/'+filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "hLsUZ1ZnBp5P"
   },
   "source": [
    "## Experiment with combination of the amount of runs from feature selection and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "starting,16,selected,1800\nlen col 128\nrunning AutoCatBoostRegressor\nbest score     -0.11809946496795386\nbest score     -0.11704481713361353\nbest score     -0.11704481713361353\nbest score     -0.11704481713361353\nbest score     -0.11704481713361353\nbest score     -0.11659591829475864\nbest score     -0.11659591829475864\nbest score     -0.11659591829475864\nbest score     -0.11659591829475864\nbest score     -0.11659591829475864\nbest score     -0.11619371202888568\nbest score     -0.11619371202888568\nbest score     -0.11619371202888568\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nbest score     -0.11608735102294879\nrunning ElasticNet\nbest score     -0.1623582094488026\nbest score     -0.12709131411198082\nbest score     -0.12113111156011624\nbest score     -0.12113111156011624\nbest score     -0.12113111156011624\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11948746525229874\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11784139503683638\nbest score     -0.11762865266003288\nbest score     -0.11762865266003288\nbest score     -0.11762865266003288\nbest score     -0.11762865266003288\nbest score     -0.11762865266003288\nbest score     -0.11762865266003288\nbest score     -0.11762865266003288\nbest score     -0.11762865266003288\nbest score     -0.11762865266003288\nbest score     -0.11762865266003288\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11751107538657397\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11721296815029576\nbest score     -0.11720081083523934\nbest score     -0.11720081083523934\nbest score     -0.11720081083523934\nbest score     -0.11720081083523934\nbest score     -0.11720081083523934\nbest score     -0.11716844857345891\nbest score     -0.11716844857345891\nbest score     -0.11716844857345891\nbest score     -0.11716844857345891\nbest score     -0.11716844857345891\nbest score     -0.11704457226250443\nbest score     -0.11704191766775451\nbest score     -0.11704191766775451\nbest score     -0.11704191766775451\nbest score     -0.11703238330072382\nbest score     -0.11703238330072382\nbest score     -0.11703238330072382\nbest score     -0.11703238330072382\nbest score     -0.11703238330072382\nbest score     -0.11701221302997752\nbest score     -0.11701221302997752\nbest score     -0.11701221302997752\nbest score     -0.11701221302997752\nbest score     -0.11701221302997752\nbest score     -0.11701221302997752\nbest score     -0.11701221302997752\nbest score     -0.11701221302997752\nbest score     -0.11701221302997752\nbest score     -0.11701221302997752\nbest score     -0.11701221302997752\nbest score     -0.11697426187242162\nbest score     -0.11697426187242162\nbest score     -0.11697426187242162\nbest score     -0.11697426187242162\nbest score     -0.11697426187242162\nbest score     -0.11697426187242162\nbest score     -0.11697426187242162\nbest score     -0.11697426187242162\nbest score     -0.116948754539773\nbest score     -0.116948754539773\nbest score     -0.116948754539773\nbest score     -0.116948754539773\nbest score     -0.116948754539773\nbest score     -0.116948754539773\nbest score     -0.116948754539773\nrunning KernelRidge\nbest score     -0.2505605301271433\nbest score     -0.16718642309327372\nbest score     -0.16718642309327372\nbest score     -0.16718642309327372\nbest score     -0.16718642309327372\nbest score     -0.16718642309327372\nbest score     -0.16718642309327372\nbest score     -0.16718642309327372\nbest score     -0.16718642309327372\nbest score     -0.16718642309327372\nbest score     -0.16181351291512533\nbest score     -0.16181351291512533\nbest score     -0.12818514431579836\nbest score     -0.122054553929704\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nbest score     -0.12199557139509515\nrunning Lasso\nbest score     -0.2045659521303265\nbest score     -0.18468889113833895\nbest score     -0.12060324279602011\nbest score     -0.12060324279602011\nbest score     -0.12060324279602011\nbest score     -0.12060324279602011\nbest score     -0.12060324279602011\nbest score     -0.12060324279602011\nbest score     -0.12060324279602011\nbest score     -0.12060324279602011\nbest score     -0.12060324279602011\nbest score     -0.11769560330659151\nbest score     -0.11769560330659151\nbest score     -0.11769560330659151\nbest score     -0.11769560330659151\nbest score     -0.11769560330659151\nbest score     -0.11769560330659151\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nbest score     -0.1154191898315362\nrunning xgb.XGBRegressor\nbest score     -0.1247046985162847\nbest score     -0.1247046985162847\nbest score     -0.1247046985162847\nbest score     -0.1247046985162847\nbest score     -0.1247046985162847\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nbest score     -0.12358486268398836\nrunning lgb.LGBMRegressor\nbest score     -0.12358544173846177\nbest score     -0.12358544173846177\nbest score     -0.1230277423357494\nbest score     -0.1230277423357494\nbest score     -0.1230277423357494\nbest score     -0.1230277423357494\nbest score     -0.1224069268778635\nbest score     -0.1224069268778635\nbest score     -0.1224069268778635\nbest score     -0.1224069268778635\nbest score     -0.1224069268778635\nbest score     -0.1224069268778635\nbest score     -0.1224069268778635\nbest score     -0.1224069268778635\nbest score     -0.12114962441429976\nbest score     -0.12114962441429976\nbest score     -0.12114962441429976\nbest score     -0.12114962441429976\nbest score     -0.12099775338889991\nbest score     -0.12099775338889991\nbest score     -0.12099775338889991\nbest score     -0.12099775338889991\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12092530852969573\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nbest score     -0.12036421445040765\nhyperparameter tuning done\nAutoCatBoostRegressor          Best score 0.1161 std 0.0094 train 0.0311 time 0.0560\nElasticNet                     Best score 0.1169 std 0.0143 train 0.0949 time 0.1980\nKernelRidge                    Best score 0.1220 std 0.0137 train 0.1046 time 0.1934\nLasso                          Best score 0.1154 std 0.0159 train 0.0946 time 0.1796\nxgb.XGBRegressor               Best score 0.1236 std 0.0092 train 0.0855 time 0.1068\nlgb.LGBMRegressor              Best score 0.1204 std 0.0106 train 0.0540 time 0.1102\nNone\nstacking model train 0.0540, test 0.1133\n[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n6\nstacking model train 0.0583, test 0.1182\n[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n6\nstacking model train 0.0589, test 0.1234\n[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n6\nstacking model train 0.0595, test 0.1244\n[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n6\nstacking model train 0.0607, test 0.1186\n[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n6\n"
    }
   ],
   "source": [
    "feat_selector = load('output/feat_selector.pickle')\n",
    "for run_amount, strictness in [(16,'selected')]:\n",
    "    for train_time in [1800]:\n",
    "        print(f'starting,{run_amount},{strictness},{train_time}')\n",
    "        cols = set(feat_selector.named_steps['model'].runs[run_amount][strictness])\n",
    "        print('len col', len(cols))\n",
    "        to_test = [k for k in models]\n",
    "        to_test = [\n",
    "            'AutoCatBoostRegressor',\n",
    "            'ElasticNet',\n",
    "            'KernelRidge',\n",
    "            'Lasso',\n",
    "            'xgb.XGBRegressor',\n",
    "            'lgb.LGBMRegressor']\n",
    "        results = {}\n",
    "        for name in to_test:\n",
    "            results[name] = optimize_model(name, train_x, train_time)\n",
    "        save(results,'hyperparam_tuning'+str(run_amount)+strictness+str(train_time))\n",
    "        print('hyperparameter tuning done')\n",
    "        print(print_results(results))\n",
    "        for rand_seed in [64,128,42,44,22]:\n",
    "            model = get_stacked_model(results,train_x=train_x[cols],rand_seed=rand_seed)\n",
    "            submit(model, 'submission'+str(run_amount)+strictness+str(train_time)+str(rand_seed)+'csv')\n",
    "            save(model, 'ensemble'+str(run_amount)+strictness+str(train_time)+str(rand_seed)+'pickle')\n",
    "            blend(model,'blend'+str(run_amount)+strictness+str(train_time)+str(rand_seed)+'.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Copy of rentJesse.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('ProgramData': virtualenv)",
   "language": "python",
   "name": "python37364bitprogramdatavirtualenv99403c2e8abd4ba0909557516bfee9d9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}