{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "_NDWSAWprShN",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import altair as alt\n",
    "from aoc import timeit\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def load(filename):\n",
    "    f = open(filename,\"rb\")\n",
    "    return pickle.load(f)\n",
    "    \n",
    "def save(model, filename='bestmodel.pickle'):\n",
    "    with open('output/'+filename, 'wb') as handle:\n",
    "        pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_feature_selection(cols, filename='feat_selection.pickle'):\n",
    "    with open('output/'+filename, 'wb') as handle:\n",
    "        pickle.dump(cols, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def submit(model):\n",
    "    pred = model.predict(final_test)\n",
    "    final_test['SalePrice'] = np.exp(pred)\n",
    "    final_test[['Id','SalePrice']].to_csv('output/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "colab_type": "code",
    "id": "3whduyAbrShU",
    "outputId": "ee37d5cf-5d0f-4de2-c51b-9a622e6998a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open(\"output/engineered_datasets.pickle\",\"rb\")\n",
    "train_x, train_y, final_test, num_x, cat_x, cat_x_ind = pickle.load(f)\n",
    "f = open(\"output/feat_selection.pickle\",\"rb\")\n",
    "cols = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "BPXAZImebs9C"
   },
   "source": [
    "## Import preprocessing pipelines & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from utils.sklearn_custom_steps import DFSimpleImputer, DFOneHotEncoder,DFMinMaxScaler,DFColumnTransformer,DFOutlierExtractor,DFOutlierExtractor,DFStandardScaler,DFRobustScaler,DFSmartImputer, DFUnSkewer, DFPowerTransformer\n",
    "from utils.sklearn_custom_steps import get_pipeline\n",
    "from utils.model_hyperparameters import models\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet,SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler\n",
    "from utils.model_hyperparameters import AutoCatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_models(to_test,train_x=train_x,**kwargs):\n",
    "    for name in to_test:\n",
    "        print(f\"{name.ljust(20)}\", end = ': ')\n",
    "        pipe = get_pipeline(models[name].model, **models[name].preprocess, **kwargs)\n",
    "        test_pipeline(pipe, train_x = train_x)\n",
    "         \n",
    "def test_model(model,train_x = train_x,param=None):\n",
    "    if not param: param = {}\n",
    "    pipe = get_pipeline(model,**param)\n",
    "    return test_pipeline(pipe, train_x=train_x)\n",
    "\n",
    "def test_pipeline(pipe,train_x = train_x):\n",
    "    # print(train_x.shape)\n",
    "    num_fold = 5\n",
    "    \n",
    "    scores = cross_validate(pipe, train_x, train_y, scoring='neg_root_mean_squared_error', cv=num_fold, return_train_score=True)\n",
    "    print(f\"test {-1 * sum(scores['test_score'])/num_fold:.7f}, train {-1 * sum(scores['train_score'])/num_fold:.7f}\")\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to hyperparameter search on preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output/hyperparam_tuning.pickle\",\"rb\")\n",
    "results = pickle.load(f)\n",
    "def get_estimator(model_name, results):\n",
    "    model = get_pipeline(models[model_name].model, **models[model_name].preprocess)\n",
    "    model.set_params(**results[model_name].best_params_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "start AutoCatBoostRegressor\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed: 22.1min finished\nstart ElasticNet\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  1.0min finished\nstart KernelRidge\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  1.2min finished\nstart Lasso\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:   37.2s finished\nstart xgb.XGBRegressor\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed: 27.3min finished\nstart lgb.LGBMRegressor\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  2.8min finished\n"
    }
   ],
   "source": [
    "def hyperparam_search_pipeline(model_name, pipe):\n",
    "   print('start', model_name)\n",
    "   param_grid = {\n",
    "      # 'preprocess__col_trans__category_cat_to_num': [DFOneHotEncoder(handle_unknown=\"ignore\")],\n",
    "      # 'preprocess__col_trans__numeric__unskew_num' : [DFUnSkewer(),'passthrough'],\n",
    "      'preprocess__col_trans__numeric__scale_num' : [DFStandardScaler(),DFRobustScaler(),DFMinMaxScaler(),DFPowerTransformer()],\n",
    "      'preprocess__col_trans__numeric__impute_num__strategy': ['mean','median','most_frequent'],\n",
    "      'preprocess__col_trans__category__impute_cat__strategy': ['most_frequent','constant']}\n",
    "   search = GridSearchCV(pipe, param_grid, cv=5,scoring='neg_root_mean_squared_error',verbose=1).fit(train_x[cols], train_y)\n",
    "   frame =pd.DataFrame(search.cv_results_)\n",
    "   frame.sort_values(by='rank_test_score', inplace=True)\n",
    "   return frame\n",
    "pipe_search = dict()\n",
    "for model_name in results:\n",
    "   pipe_search[model_name] = hyperparam_search_pipeline(model_name, get_estimator(model_name, results))\n",
    "   save(pipe_search,  'hyperparam_pipe.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "AutoCatBoostRegressor\nbest score -0.1153902521105318\nbest params constant mean DFMinMaxScaler\nElasticNet\nbest score -0.11587168950921545\nbest params most_frequent median DFStandardScaler\nKernelRidge\nbest score -0.11674511912812666\nbest params most_frequent median DFStandardScaler\nLasso\nbest score -0.1158660391027462\nbest params most_frequent median DFStandardScaler\nxgb.XGBRegressor\nbest score -0.11712434284626357\nbest params most_frequent most_frequent DFMinMaxScaler\nlgb.LGBMRegressor\nbest score -0.11844457036420222\nbest params constant median DFRobustScaler\n"
    }
   ],
   "source": [
    "pipe_search = load('output/hyperparam_pipe.pickle')\n",
    "for model_name, res in pipe_search.items():\n",
    "    res = res.reset_index()\n",
    "    print(model_name)\n",
    "    print('best score', res['mean_test_score'][0])\n",
    "    print('best params', \n",
    "        res['param_preprocess__col_trans__category__impute_cat__strategy'][0],\n",
    "        res['param_preprocess__col_trans__numeric__impute_num__strategy'][0],\n",
    "        res['param_preprocess__col_trans__numeric__scale_num'][0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Test of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "AutoCatBoostRegressor\ntest 0.1167024, train 0.0267797\nElasticNet\ntest 0.1176089, train 0.0997127\nKernelRidge\ntest 0.1211799, train 0.1044793\nLasso\ntest 0.1175548, train 0.0994643\nxgb.XGBRegressor\ntest 0.1217188, train 0.0448069\nlgb.LGBMRegressor\ntest 0.1217541, train 0.0508283\n"
    }
   ],
   "source": [
    "# full dataset\n",
    "for model_name in results:\n",
    "    print(model_name)\n",
    "    test_pipeline(get_estimator(model_name, results),train_x=train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "AutoCatBoostRegressor\ntest 0.1153903, train 0.0287469\nElasticNet\ntest 0.1158717, train 0.1040336\nKernelRidge\ntest 0.1167451, train 0.1066567\nLasso\ntest 0.1158660, train 0.1038959\nxgb.XGBRegressor\ntest 0.1171243, train 0.0469092\nlgb.LGBMRegressor\ntest 0.1184446, train 0.0548421\n"
    }
   ],
   "source": [
    "# selected columns dataset\n",
    "for model_name in results:\n",
    "    print(model_name)\n",
    "    test_pipeline(get_estimator(model_name, results),train_x=train_x[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "wO5qirPMBqGX"
   },
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "running AutoCatBoostRegressor\nbest score     -0.11642536345507444\nbest score     -0.11642536345507444\nbest score     -0.11642536345507444\nbest score     -0.11642536345507444\nbest score     -0.11642536345507444\nbest score     -0.11541756067751201\nbest score     -0.11541756067751201\nbest score     -0.11541756067751201\nbest score     -0.11541756067751201\nbest score     -0.11541756067751201\nbest score     -0.11541756067751201\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nbest score     -0.1153902521105318\nrunning ElasticNet\nbest score     -0.15391917297445676\nbest score     -0.13207174365947225\nbest score     -0.11997953078093863\nbest score     -0.11997953078093863\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11696460440480955\nbest score     -0.11622250855259453\nbest score     -0.11622250855259453\nbest score     -0.11622250855259453\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.1159107419707356\nbest score     -0.11587823638063209\nbest score     -0.11587823638063209\nbest score     -0.11587823638063209\nbest score     -0.11587823638063209\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587819791017108\nbest score     -0.11587808452933457\nbest score     -0.11587808452933457\nbest score     -0.11587808452933457\nbest score     -0.11587775451441185\nbest score     -0.11587775451441185\nbest score     -0.11587674135995629\nbest score     -0.11587674135995629\nbest score     -0.11587674135995629\nbest score     -0.11587674135995629\nbest score     -0.11587674135995629\nbest score     -0.11587584617903963\nbest score     -0.11587584617903963\nbest score     -0.11587584617903963\nbest score     -0.11587584617903963\nbest score     -0.11587584617903963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587470115080963\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11587267570059773\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nbest score     -0.11586105587868716\nrunning KernelRidge\nbest score     -0.11749494293370243\nbest score     -0.11749494293370243\nbest score     -0.11684151236933828\nbest score     -0.11684151236933828\nbest score     -0.11684151236933828\nbest score     -0.11684151236933828\nbest score     -0.11684151236933828\nbest score     -0.11684151236933828\nbest score     -0.11684151236933828\nbest score     -0.11684151236933828\nbest score     -0.11684151236933828\nbest score     -0.11674699889997074\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nbest score     -0.11674511912812668\nrunning Lasso\nbest score     -0.1407673972831336\nbest score     -0.1331972853733995\nbest score     -0.1303623303904965\nbest score     -0.1303623303904965\nbest score     -0.12016918911981252\nbest score     -0.12016918911981252\nbest score     -0.11595362895115209\nbest score     -0.11595362895115209\nbest score     -0.11595362895115209\nbest score     -0.11595362895115209\nbest score     -0.11595362895115209\nbest score     -0.11595362895115209\nbest score     -0.11590900017285878\nbest score     -0.11590534415547497\nbest score     -0.11590096946001999\nbest score     -0.11589381328157485\nbest score     -0.11589381328157485\nbest score     -0.11589381328157485\nbest score     -0.1158733739338837\nbest score     -0.1158733739338837\nbest score     -0.11587096876340794\nbest score     -0.11587096876340794\nbest score     -0.11587096876340794\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nbest score     -0.11586537265343808\nrunning xgb.XGBRegressor\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nbest score     -0.11712434284626357\nrunning lgb.LGBMRegressor\nbest score     -0.1252952524346242\nbest score     -0.12067144956853738\nbest score     -0.12067144956853738\nbest score     -0.12067144956853738\nbest score     -0.12067144956853738\nbest score     -0.12067144956853738\nbest score     -0.12067144956853738\nbest score     -0.12034715849736626\nbest score     -0.12034715849736626\nbest score     -0.12034715849736626\nbest score     -0.12034715849736626\nbest score     -0.12034715849736626\nbest score     -0.12034715849736626\nbest score     -0.12034715849736626\nbest score     -0.12034715849736626\nbest score     -0.12034715849736626\nbest score     -0.12034715849736626\nbest score     -0.11927456203896905\nbest score     -0.11927456203896905\nbest score     -0.11927456203896905\nbest score     -0.11927456203896905\nbest score     -0.11847454581019114\nbest score     -0.11847454581019114\nbest score     -0.11847454581019114\nbest score     -0.11840502402849432\nbest score     -0.11840502402849432\nbest score     -0.11840502402849432\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\nbest score     -0.11832086367497666\n"
    }
   ],
   "source": [
    "from skopt import BayesSearchCV, callbacks\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "TRAIN_TIME = 1500\n",
    "NUM_ITERATIONS = 150\n",
    "NO_IMPROVEMENT_STOP_THRES = 25\n",
    "\n",
    "def gen_opt_settings(model_name):\n",
    "    model = {'model': [models[model_name].model]}\n",
    "    for k,v in models[model_name].hyper.items():\n",
    "        model['model__'+k] = v\n",
    "    if models[model_name].hyper:\n",
    "        return (model, NUM_ITERATIONS)\n",
    "    else:\n",
    "        return (model, 1)\n",
    "\n",
    "def optimize_model(model_name,train_x = train_x):\n",
    "    print('running', model_name)\n",
    "    def no_improvement_detector(optim_result):\n",
    "        score = opt.best_score_\n",
    "        # print(optim_result.x)\n",
    "        print(f\"{'best score':15}{score}\")\n",
    "        if score > opt.train_status['current_score']:\n",
    "            opt.train_status['current_score'] = score\n",
    "            opt.train_status['not_improving'] = 0\n",
    "        else:\n",
    "            opt.train_status['not_improving'] += 1\n",
    "            if opt.train_status['not_improving'] == opt.train_status['stop_thres']: return True\n",
    "    checkpointsaver = callbacks.CheckpointSaver(\"output/\" + model_name + \"_skopt.pkl\")\n",
    "    deadlinestopper = callbacks.DeadlineStopper(TRAIN_TIME)\n",
    "\n",
    "    opt = BayesSearchCV(\n",
    "        get_pipeline(models[model_name].model, **models[model_name].preprocess),\n",
    "        [gen_opt_settings(model_name)],\n",
    "        cv=5, \n",
    "        scoring = 'neg_root_mean_squared_error',\n",
    "        return_train_score = True,\n",
    "        random_state = 112,\n",
    "        refit=False\n",
    "        )\n",
    "    opt.train_status = { 'current_score': -100, 'not_improving': 0, 'stop_thres' :NO_IMPROVEMENT_STOP_THRES}\n",
    "    opt.fit(train_x,train_y, callback = [no_improvement_detector,checkpointsaver,deadlinestopper])\n",
    "    return opt\n",
    "\n",
    "def hashing(self): return 8398398478478 \n",
    "CatBoostRegressor.__hash__ = hashing # otherwise skopt flips\n",
    "\n",
    "to_test = [k for k in models]\n",
    "to_test = [\n",
    "    'AutoCatBoostRegressor',\n",
    "    'ElasticNet',\n",
    "    'KernelRidge',\n",
    "    'Lasso',\n",
    "    'xgb.XGBRegressor',\n",
    "    'lgb.LGBMRegressor']\n",
    "results = {}\n",
    "for name in to_test:\n",
    "    results[name] = optimize_model(name, train_x[cols])\n",
    "save(results,'hyperparam_tuning.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output/hyperparam_tuning.pickle\",\"rb\")\n",
    "results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "AutoCatBoostRegressor          Best score 0.1154 std 0.0062 train 0.0287 time 0.0950\nElasticNet                     Best score 0.1159 std 0.0120 train 0.1038 time 0.1147\nKernelRidge                    Best score 0.1167 std 0.0104 train 0.1067 time 0.1265\nLasso                          Best score 0.1159 std 0.0119 train 0.1039 time 0.0852\nxgb.XGBRegressor               Best score 0.1171 std 0.0109 train 0.0469 time 0.0592\nlgb.LGBMRegressor              Best score 0.1183 std 0.0088 train 0.0675 time 0.0702\n"
    }
   ],
   "source": [
    "#summarize tuning results\n",
    "for model in results:\n",
    "    best_run = results[model].cv_results_['rank_test_score'].index(1)\n",
    "    mean_test_score = -1 * results[model].cv_results_['mean_test_score'][best_run]\n",
    "    std_test_score = results[model].cv_results_['std_test_score'][best_run]\n",
    "    mean_train_score = -1 * results[model].cv_results_['mean_train_score'][best_run]\n",
    "    mean_score_time = results[model].cv_results_['mean_score_time'][best_run]\n",
    "    best_params = results[model].best_params_\n",
    "    print(f\"{model:<30} Best score {mean_test_score:.4f} std {std_test_score:.4f} train {mean_train_score:.4f} time {mean_score_time:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoCatBoostRegressor          Best score 0.1154 std 0.0062 train 0.0287 time 0.0474\n",
    "ElasticNet                     Best score 0.1159 std 0.0119 train 0.1040 time 0.1059\n",
    "KernelRidge                    Best score 0.1168 std 0.0104 train 0.1067 time 0.1179\n",
    "Lasso                          Best score 0.1159 std 0.0119 train 0.1039 time 0.0895\n",
    "xgb.XGBRegressor               Best score 0.1177 std 0.0090 train 0.0468 time 0.1028\n",
    "lgb.LGBMRegressor              Best score 0.1190 std 0.0085 train 0.0544 time 0.0870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Stacking best models from hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "stacking model train 0.0697, test 0.1108\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "def get_estimator(model_name):\n",
    "    model = get_pipeline(models[model_name].model, **models[model_name].preprocess)\n",
    "    model.set_params(**results[model_name].best_params_)\n",
    "    return model\n",
    "\n",
    "to_stack_list = [\n",
    "    'AutoCatBoostRegressor',\n",
    "    'ElasticNet',\n",
    "    'KernelRidge',\n",
    "    'Lasso',\n",
    "    'xgb.XGBRegressor',\n",
    "    'lgb.LGBMRegressor']\n",
    "\n",
    "# to_stack_list = to_test\n",
    "# to_stack = [(model_name, results[model_name].best_estimator_) for model_name in to_stack_list]\n",
    "# to_stack = [(model_name, results[model_name].best_estimator_) for model_name in results]\n",
    "to_stack = [(model_name, get_estimator(model_name)) for model_name in to_stack_list]\n",
    "model = StackingRegressor(to_stack, passthrough = False)\n",
    "num_fold = 5\n",
    "scores = cross_validate(model, train_x[cols], train_y, scoring='neg_root_mean_squared_error', cv=num_fold, return_train_score=True)\n",
    "print(f\"stacking model train {-1 * sum(scores['train_score'])/num_fold:.4f}, test {-1 * sum(scores['test_score'])/num_fold:.4f}\")\n",
    "model.fit(train_x[cols],train_y)\n",
    "save(model,'ensemble.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending, to get rid of some of the overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output/ensemble.pickle\",\"rb\")\n",
    "model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n 0.14285714]\n7\n"
    }
   ],
   "source": [
    "preds = [estimator.predict(final_test) for estimator in model.estimators_]\n",
    "weights = np.array([1/7]*len(preds) + [(1-len(preds)*(1/7))])\n",
    "# weights = np.array([1/6]*len(preds))\n",
    "print(weights)\n",
    "preds.append(model.predict(final_test))\n",
    "print(len(preds))\n",
    "# weigh the individual models with 0.1 and the stacked regressor with the remainder\n",
    "weighted_preds = preds * weights[:, None]\n",
    "final_preds = np.sum(weighted_preds,axis=0)\n",
    "final_test['SalePrice'] = np.exp(final_preds)\n",
    "final_test[['Id','SalePrice']].to_csv('output/blend_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OrderedDict([('model', XGBRegressor(base_score=None, booster=None,\n                           colsample_bylevel=0.8796707319903774, colsample_bynode=None,\n                           colsample_bytree=0.9954194772260316, gamma=0.009380863327905845,\n                           gpu_id=None, importance_type='gain', interaction_constraints=None,\n                           learning_rate=0.032237466373952015,\n                           max_delta_step=4.320822620313757, max_depth=4, min_child_weight=1,\n                           missing=nan, monotone_constraints=None, n_estimators=3827,\n                           n_jobs=None, nthread=-1, num_parallel_tree=None,\n                           objective='reg:squarederror', random_state=7, reg_alpha=0.464,\n                           reg_lambda=0.4614615602391834, scale_pos_weight=None, silent=True,\n                           subsample=0.6426784642167161, tree_method=None,\n                           validate_parameters=False, verbosity=None)),\n             ('model__colsample_bylevel', 0.8796707319903774),\n             ('model__colsample_bytree', 0.9954194772260316),\n             ('model__gamma', 0.009380863327905845),\n             ('model__learning_rate', 0.032237466373952015),\n             ('model__max_delta_step', 4.320822620313757),\n             ('model__max_depth', 4),\n             ('model__min_child_weight', 1),\n             ('model__n_estimators', 3827),\n             ('model__reg_lambda', 0.4614615602391834),\n             ('model__subsample', 0.6426784642167161)])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "results['xgb.XGBRegressor'].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "hLsUZ1ZnBp5P"
   },
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "1wvgdkTRBptT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = get_pipeline(CatBoostRegressor(silent=True,cat_features=cat_x),onehot=False)\n",
    "# model = model.fit(train_x,train_y)\n",
    "submit(model)\n",
    "save(model,'ensemble.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Copy of rentJesse.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('ProgramData': virtualenv)",
   "language": "python",
   "name": "python37364bitprogramdatavirtualenv99403c2e8abd4ba0909557516bfee9d9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}