I've spend quite some time (guess at least 50 hours) with the dataset and wanted to share some experiences. My final score is 0.11711, which is top 2-3%. My final approach is simply a blend of 6 models weighted equally.

Top 8 Insights
1) Getting a pipeline up and running fast is great to get started. I recommend trying out some different types of models that work out of the box, like LassoCV or a gradient boosting model such as XGBoost. Using Lasso for example won't work since you need to tune the alpha parameter for good results.
2) Feature engineering is king. I used most of the features other people used in their kernels and it's really important for the score. Also using log and quadratic terms works. Experimented with using categorical embeddings instead of one-hot encoding, didn't work.
3) I tuned a wide collection of algorithms and selected the ones that performed best for ensembling. Did the tuning with Skopt which after some fiddling worked nicely.
4) Regularized linear models and boosting trees work best. I used Lasso, ElasticNet, KernelRidge, Catboost, LGB and XGB in the ensemble. Neural nets didn't work for me, but didn't exhaustively test.
5) I hacked around a bit and this was good fun. Some examples: monkeypatched CatBoost so it could take categorical features without needing to specify them when fitting with a cat_features parameter. This made it play nice with sklearn pipeline. Also I wrote a function that returned a sklearn pipeline with the exact imputation and scaling that I wanted to have. Lastly I subclassed sklearn preprocessing steps into returning a DataFrame instead of an array.
6) Optimizing the preprocessing pipeline (e.g. what type of imputing to do, and which scaling) helps a bit but not too much. Spend quite some time there. Will come back to feature selection in the difficulties below.
7) Many kernels on kaggle impute on the whole dataset, which can result in data leakage (information about the validation set leaking into the training set, inflating your validation score). I didn't do it and still managed to get a nice score. For example: instead of unskewing skewed variables on the full dataset you can also run an sklearn StandardScaler in a pipeline. This will fit on the train set and prevent data leakage.
8) Jupyter notebooks are great. For me splitting the workbooks in separate files worked well. It keeps them from slowing down. This investment of time in refractoring really payed off. Also I refractored some workbooks into .py files (such as hyperparameters to test)

Top 4 learnings
1) Be very, very careful removing outliers. I've seen so many notebooks that removed outliers, but in my experience it only lowers performance. And bigtime as in +0.007 increase. In addition, it screws up your cross-validation.
2) Blending models really helps. For me a blended model outperforms single or stacked models by 0.005, which is quite a lot on the LB. For me a surprise was that including my stacked model in the blend led to a lower score on the LB.
3) Version control & reproducibility is very important. I've spend quite some time trying to reproduce a top performing score which I lost somehow. It nearly let me to throw the towel way above my final score. Git has been of great help here. Also naming my output files helped. E.g. not model.pickle, but model_10mintuning_100cols_blend6models.pickle. Finally I made a major error: found out I dindn't consistently use a random seed. This hurts reproducibility. I ran some experiments and due to random seeds I've probably overfitted on the LB by about 0.001. Basically being lucky with the random numbers in tuning my hyperparameters and fitting the models. I tuned hyperparameters with identical settings, the range between best and worst submission was 0.0025!
4) When to stop? Don't have a verdict yet, but I've spend so much time here. On one hand keep on pushing has led to some breakthroughs and made me have a result I'm very happy with. But when to stop? I really could continue experimenting on this competition for a couple of weeks. But reflecting on it, instead of trying to get the last 0.0005 increase on the LB, probably I'll learn more with tackling a new problem such as time series or something with image recognition (trying to learn Pytorch).

2 Open Questions
1) Feature selection. I did this after feature engineering with the Boruta approach. This tries to filter out columns without predictive value. It worked, but overdoing it quickly leads to detrimental results. So I was quite conservative with removing them and ended up with final dataset of 100 columns). I'm still not exactly sure how to fully optimize this. Difficult interpretation of the results was also due to my lack of setting random seeds.
2) The big one: My cross validation wasn't matching the LB score and sometimes a better CV led to a lower LB score. In general it was fine, but when trying to optimize it's also about the 4th decimal that you want to correspond, not only the first 3. Probably has something to do with this being a relatively small dataset, where some data points in the test set have a large impact on your score. Also another big takeaway was that using Kfold with shuffle=True underestimated my LB score and shuffle=False overestimated the score. Example LB score 0.121, CV shuffle=False 0.11, CV shuffle=True 0.124

Overall learned a lot. Hope it helps & look forward to your insights, especially on the CV difficulty!

Code can be found on my github, https://github.com/jvanelteren/housing/ since it's split into several notebooks.

Thanks for the support,
Jesse
