I've spend quite some time (guess at least 50 hours) with this competition and wanted to share some experiences. My final score is 0.11711, which is top 2-3%. My final approach is simply a blend of 6 models weighted equally.

Top 8 Insights
1) Getting a pipeline up and running fast is great to get started. I recommend trying out some different types of models that work out of the box, like LassoCV or a gradient boosting model such as XGBoost. Using Lasso for example won't work since you need to tune the alpha parameter for good results.
2) Feature engineering is king. I used most of the features other people used in their kernels and it's really important for the score. Also using log and quadratic terms works. Experimented with using categorical embeddings instead of one-hot encoding, this didn't work.
3) Skopt is better then GridSearchCV since it draws from a distribution instead of mindless trying all the settings.
4) Regularized linear models and boosting trees work best. I used Lasso, ElasticNet, KernelRidge, Catboost, LGB and XGB in the ensemble. Neural nets didn't work for me, but didn't exhaustively test.
5) Hacking around a bit when things don't work out of the box is good fun. Some examples: monkeypatched CatBoost so it could take categorical features without needing to specify them when fitting with a cat_features parameter. This made it play nice with sklearn pipeline. Also I wrote a function that returned a sklearn pipeline with the exact imputation and scaling that I wanted to have. Lastly I subclassed sklearn preprocessing steps into returning a DataFrame instead of an array.
6) Optimizing the preprocessing pipeline (e.g. what type of imputing to do, and which scaling) didn't do much for my score, e.g. imputing median or mean, or which scaler to use. Spend quite some time there, didn't pay off.
7) Many kernels on kaggle impute on the whole dataset, which can result in data leakage (information about the validation set leaking into the training set, inflating your validation score). I didn't do it and still managed to get a nice score. For example: instead of unskewing skewed variables on the full dataset you can also run an sklearn StandardScaler in a pipeline. This will fit on the train set and prevent data leakage.
8) Splitting a notebook in separate files worked well when the codebase grows. It keeps Jupyter to grinding to a halt. This investment of time in refractoring really payed off. Also I refractored some workbooks into .py files to import (such as a file with all the hyperparameters to test)

Top 4 learnings
1) Be very, very careful removing outliers. I've seen so many notebooks that removed outliers, but in my experience it only lowers performance. And bigtime as in +0.007 increase. In addition, it screws up your cross-validation.
2) Blending models really helps. For me a blended model outperforms single or stacked models by 0.005, which is quite a lot on the LB. For me a surprise was that including my stacked model in the blend led to a lower score on the LB.
3) Version control & reproducibility is very important. I've spend quite some time trying to reproduce a top performing score which I lost somehow. It nearly let me to throw the towel way above my final score. Git has been of great help here. Also naming my output files helped. E.g. not model.pickle, but model_10mintuning_100cols_blend6models.pickle. Finally I made a major error: found out I dindn't consistently use a random seed. This hurts reproducibility. I ran some experiments and solely due to random seeds there can be around 0.002 difference in score. Basically being lucky with the random numbers fitting the models makes the difference between a good score and a great score. My score with averaging some random seeds was 0.11741. This is the most principled approach instead of getting lucky with a random seed imo.
4) When to stop with the competition? Don't have a verdict yet, but I've spend so much time here. On one hand keep on pushing has led to some breakthroughs & insights and made me reach a result I'm very happy with. But when to stop? I really could continue experimenting on this competition for a couple of weeks. But reflecting on it, instead of trying to get the last 0.0005 increase on the LB, probably I'll learn more with tackling a new problem such as something with image recognition (trying to learn Pytorch).

2 Open Questions
1) Feature selection. I did this after feature engineering with the Boruta approach. This tries to filter out columns without predictive value. It worked, but overdoing it quickly leads to detrimental results. So I was quite conservative with removing them and ended up with final dataset of 128 columns). I'm still not exactly sure how to fully optimize this. Difficult interpretation of the results was also due to my lack of setting random seeds, didn't redo it afterwards.
2) The big one: My cross validation wasn't matching the LB score well enough. Sometimes a better CV led to a lower LB score. When trying to optimize you just want it to match. Probably has something to do with this being a relatively small dataset, where some data points in the test set (outliers?) have a large impact on your score. Also another big takeaway was that using Kfold with shuffle=True results in a worse CV score then shuffle=False. 

Look forward to your insighs (especially on the cross validation)!

Hope this helps, thanks for the support,
Jesse

Code can be found on Github, https://github.com/jvanelteren/housing/ since it's split into several notebooks.